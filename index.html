<!doctype html>
<html lang="en" class="h-100">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Research project">
    <meta name="author" content="Simon Dobnik">
    <meta name="generator" content="Hugo 0.84.0">
    <title>Project description: Beyond Pixels and Words (BPW)</title>

    <link rel="canonical" href="https://sdobnik.github.io/">

    

    <!-- Bootstrap core CSS -->
    <!-- <link href="../assets/dist/css/bootstrap.min.css" rel="stylesheet"> -->
 <link href="bootstrap-5.3.3/css/bootstrap.min.css" rel="stylesheet"> 

    <style>
      .bd-placeholder-img {
        font-size: 1.125rem;
        text-anchor: middle;
        -webkit-user-select: none;
        -moz-user-select: none;
        user-select: none;
      }

      @media (min-width: 768px) {
        .bd-placeholder-img-lg {
          font-size: 3.5rem;
        }
      }
    </style>

    
    <!-- Custom styles for this template -->
    <link href="sticky-footer.css" rel="stylesheet">
  </head>
  <body class="d-flex flex-column h-100">
    
<!-- Begin page content -->
<main class="flex-shrink-0">
  <div class="container">
    <h1 class="mt-5">Beyond Pixels and Words (BPW)</h1>

    <figure class="figure">
    <img src="DALL-E-bpw.webp" class="img-fluid" alt="Description of project in an image generated by ChatGPT">
    <figcaption class="figure-caption">Generated by DALL·E/ChatGPT 2025-03-11.</figcaption>
    </figure>
    
    <p class="lead">The goal of this project is to improve on the state of the art of computational models of language and perception for interactive artificial agents in a way that would go beyond mere recognition of patterns in perceptual data. The project focuses on the domain of spatial descriptions (such as “bring me the red book on the table beside my bed”) which present many open challenges for computational modelling of language and perception. In particular, it explores new dynamic models of language interpretation and generation involving integration of information from several modalities; and new machine learning scenarios whereby the agents can improve the acquisition of the missing knowledge by directly interacting with a human partner (a situated version of active learning) or integrating knowledge that it has learned off-line in a different domain (transfer learning).</p>
    
    <h2 class="mt-5" id="why-computational-modelling-of-language">Why computational modelling of spatial language?</h2>

    <p>Research shows that computational modelling of spatial language requires integration of different sources of knowledge that affect the semantics of spatial descriptions: identification of objects, arrangements of objects and scene geometry, knowledge about dynamic kinematic routines between objects, and language coordination with dialogue partners.</p>

    <p>However, several open questions still remain about how multi-modal representations are learned by particular deep learning architectures and what improvements to these architectures can be made. Because situated agents are located within realistic and changing environments, they may encounter new conversational partners, new spaces and new objects which means that both linguistic and perceptual representations must be learned continuously. What algorithm can be used for such learning and how successful they are? Another challenge for learning from the environment is also that it is limited by the number of the contexts an agent experiences in its lifetime but the agents must be minimally useful when they are first deployed. How useful is pre-trained background knowledge from another domain and how such pre-trained knowledge can be integrated within the current learning scenario?</p>

    <h2 class="mt-5" id="our-aims">Our aims</h2>

    <p>These open questions identify the following specific aims of this research project:</p>

    <p><strong>Aim 1</strong> Develop new multi-modal computational models of spatial descriptions that integrate perceptual, world knowledge and dialogue contexts using deep learning architectures.</p>

    <p><strong>Aim 2</strong> Develop computational methods of interactive learning of spatial language through the agent’s interaction with the world (perception) and other agents (dialogue) using active learning.</p>

    <p><strong>Aim 3</strong> Integrate and test deep learning models initially trained in an offline fashion and in a different domain (e.g. image description corpus) to an interactive scenario through the use of domain adaptation and transfer learning.</p>

    <p><strong>Aim 4</strong> As a proof of concept, implement the models in a (virtual) situated agent.</p>

    <p>The novelty of this project is that it treats the multimodal semantic representations underpinning spatial language as continuously adaptable and learnable from linguistic and perceptional contexts using the state-of-the-art machine learning approaches.</p>

    <h2 class="mt-5" id="work-packages">Work packages</h2>

    <p>The length of the project is 3 years during which these tasks are explored iteratively in stages, increasing the complexity of situations and learning.</p>

    <p><strong>Task 1 Computational models of spatial descriptions in interaction:</strong></p>

    <ul>
      <li>1.1 Model of objects using image data with scene geometry;</li>
      <li>1.2 Model of world knowledge;</li>
      <li>1.3 Model of attention and spatial perspective taking;</li>
    </ul>

    <p><strong>Task 2 Strategies for continuous interactive learning:</strong></p>

    <ul>
      <li>2.1 Domain evaluation and experiment design: table with objects and kinds of virtual environments and potential interactions with them;</li>
      <li>2.2 Identification of interactive strategies for continuous learning;</li>
      <li> 2.3 Extension of robotic platform(s) and/or virtual environment(s); </li>
    </ul>

    <p><strong>Task 3 Adaptation of deep learning to continuous interactive learning:</strong></p>

    <ul>
      <li>3.1 Exploration of offline corpora of language, images (with depth) and deep learning; </li>
      <li>3.2 Adaptation of deep learning algorithms for learning with limited data to our interactive scenarios (transfer learning); </li>
      <li>3.3 Adaptation of interactive deep learning algorithms (reinforcement and active learning) to our interactive scenarios; </li>
    </ul>

    <p><strong>Task 4 Evaluation of the system:</strong></p>

    <ul>
      <li> 4.1 Evaluation of interactive strategies on the rate of learning; </li>
      <li>4.2 Evaluation of pre-training on learning (transfer learning); </li>
      <li>4.3 Evaluation of the system in generating and understanding spatial language (architectures, feature representations, strategies, extrinsic performance).</li>
    </ul>

    <h2 class="mt-5" id="our-aims">Researchers</h2>

    <p>The project involves <a href="https://www.gu.se/en/about/find-staff/simondobnik">Simon Dobnik</a>, University of Gothenburg (project lead, computational models of spatial language and interaction), a researcher, and a research programmer (development and system implementation) at <a href="https://www.gu.se/en/flov">the Department of Philosophy, Linguistics and Theory of Science (FLoV)</a>, University of Gothenburg and <a href="https://www.tcd.ie/scss/people/academic-staff/kellehj3/">John Kelleher</a> (spatial language and machine learning) at <a href="https://www.tcd.ie/scss/">the School of Computer Science and Statistics</a> at Trinity College Dublin. Parts of tasks may be explored in thesis projects with masters students.</p>

    <p>The project is funded by VR Project Grant 2023-01552.</p>

    <h2 class="mt-5" id="our-aims">Contact</h2>

    <p><a href="https://www.gu.se/en/about/find-staff/simondobnik">Simon Dobnik</a> (principal investigator)</p>
    
    <p>2025-03-11</p>

  </div>
</main>

<footer class="footer mt-auto py-3 bg-light">
  <div class="container">
    <span class="text-muted">Funded by VR Project Grant 2023-01552.</span>
  </div>
</footer>


    
  </body>
</html>
